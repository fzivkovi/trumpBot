diff --git a/data_utils.py b/data_utils.py
index 0cdd31e..38c3ce0 100644
--- a/data_utils.py
+++ b/data_utils.py
@@ -42,6 +42,7 @@ PAD_ID = 0
 GO_ID = 1
 EOS_ID = 2
 UNK_ID = 3
+hyperlink = 4
 
 # Regular expressions used to tokenize.
 _WORD_SPLIT = re.compile(b"([.,!?\"':;)(])")
@@ -69,7 +70,7 @@ def spacy_tokenizer(paragraph, encoder=True):
   intermediate_words = []
 
   def processWord(word):
-    word = str(word).lower()
+    word = str(word).lower().strip()
     if 'http' in word:
       return hyperlink
     return word
@@ -77,6 +78,7 @@ def spacy_tokenizer(paragraph, encoder=True):
   doc1 = nlp(unicode(paragraph, errors='ignore'))
   for sent in doc1.sents:
     sentence = [processWord(word) for word in sent]
+    sentence = [str(word)  for word in sentence if word]
     words.extend(sentence)
     intermediate_words.append(sentence)
 
@@ -122,7 +124,7 @@ def create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,
     if len(vocab_list) > max_vocabulary_size:
       vocab_list = vocab_list[:max_vocabulary_size]
     with gfile.GFile(vocabulary_path, mode="wb") as vocab_file:
-      for w in vocab_list:
+      for i,w in enumerate(vocab_list):
         vocab_file.write(w + b"\n")
 
 
diff --git a/seq2seq_model.py b/seq2seq_model.py
index 43901c7..009d4f6 100644
--- a/seq2seq_model.py
+++ b/seq2seq_model.py
@@ -249,10 +249,10 @@ class Seq2SeqModel(object):
     for _ in xrange(self.batch_size):
 
       indexToInclude = random.choice(range(len(data[bucket_id])))
-      print('indexToInclude ')
-      print(indexToInclude)
-      print('data[bucket_id][indexToInclude] ')
-      print(data[bucket_id][indexToInclude])
+      # print('indexToInclude ')
+      # print(indexToInclude)
+      # print('data[bucket_id][indexToInclude] ')
+      # print(data[bucket_id][indexToInclude])
 
       inputs = data[bucket_id][indexToInclude]
       if len(inputs) == 2:
@@ -261,8 +261,8 @@ class Seq2SeqModel(object):
       else:
         encoder_input, decoder_input, data_weight = inputs
 
-      print('data_weight')
-      print(data_weight)
+      # print('data_weight')
+      # print(data_weight)
 
       # Encoder inputs are padded and then reversed.
       encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))
